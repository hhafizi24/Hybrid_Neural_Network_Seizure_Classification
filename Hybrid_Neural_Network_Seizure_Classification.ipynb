{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ain2FUFWBkTH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "from sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input,Dense,Dropout,Normalization,BatchNormalization,LayerNormalization\n",
        "from tensorflow.keras.layers import LSTM,GRU,Embedding,Reshape\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,Flatten\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam,Adamax\n",
        "from tensorflow.keras.regularizers import l1,l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5Gbb6umuBkTM"
      },
      "outputs": [],
      "source": [
        "# Load EEG data\n",
        "x_train = loadmat('.../x_train')['x_train']\n",
        "y_train = loadmat('.../y_train')['y_train'].T\n",
        "x_test = loadmat('.../x_test')['x_test']\n",
        "y_test = loadmat('.../y_test')['y_test'].T\n",
        "\n",
        "# Allocate 10% of train data to validation set\n",
        "x_val = x_train[-701:,:,:]\n",
        "y_val = y_train[-701:,:]\n",
        "x_train = x_train[:-701,:,:]\n",
        "y_train = y_train[:-701,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nls58awHBkTM",
        "outputId": "7d70a56b-c544-457b-f444-4c8a3559a65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape:(6310, 19, 500),      \n",
            "y_train shape:(6310, 1),      \n",
            "x_val shape:(701, 19, 500),      \n",
            "y_val shape:(701, 1),      \n",
            "x_test shape:(779, 19, 500),      \n",
            "y_test shape:(779, 1)\n",
            "\n",
            "Classes: [0 1 2 3]\n",
            "Seizure Types: \n",
            "1: for Complex Partial Seizures, 2: for Electrographic Seizures,       \n",
            "3: for Video-detected Seizures with no visual change over EEG, 0: for Normal data\n"
          ]
        }
      ],
      "source": [
        "# Print shapes of train and test sets\n",
        "print(f'x_train shape:{x_train.shape},\\\n",
        "      \\ny_train shape:{y_train.shape},\\\n",
        "      \\nx_val shape:{x_val.shape},\\\n",
        "      \\ny_val shape:{y_val.shape},\\\n",
        "      \\nx_test shape:{x_test.shape},\\\n",
        "      \\ny_test shape:{y_test.shape}\\n')\n",
        "\n",
        "# List the classes that exist within the dataset\n",
        "print(f'Classes: {np.unique(y_train)}')\n",
        "print('Seizure Types: \\n1: for Complex Partial Seizures, 2: for Electrographic Seizures,\\\n",
        "       \\n3: for Video-detected Seizures with no visual change over EEG, 0: for Normal data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3qrWX9efBkTN"
      },
      "outputs": [],
      "source": [
        "# Build the neural network model\n",
        "\n",
        "tf.random.set_seed(1234)                                   # For consistent results\n",
        "\n",
        "model = Sequential([\n",
        "    Input((x_train.shape[1],x_train.shape[2],1)),          # Input layer for EEG data: (electrodes, time points, channels)\n",
        "\n",
        "    Conv2D(8, (1,250), activation='relu', padding='same'), # Kernel length set to half the sampling rate of the data\n",
        "    BatchNormalization(),                                  # Normalize activations to stabilize training\n",
        "    AveragePooling2D((1,4)),                               # Downsample feature maps to reduce computational complexity and prevent overfitting\n",
        "    Dropout(0.2),                                          # Include dropout to prevent overfitting\n",
        "\n",
        "    Conv2D(16,(1,62), activation='relu', padding='same'),  # Additional convolutional layers to capture hierarchical features\n",
        "    BatchNormalization(),\n",
        "    AveragePooling2D((1,2)),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Conv2D(32,(1,31), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    AveragePooling2D((1,2)),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Reshape((x_train.shape[1],-1)),                        # Reshape to prepare for LSTM layer\n",
        "    LSTM(units=128, activation='relu'),\n",
        "\n",
        "    Dense(4)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1sKn3ksHBkTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 104ms/step - accuracy: 0.9781 - loss: 0.0589\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9275 - loss: 0.1975\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9358 - loss: 0.2335\n",
            "Train Loss: 0.06, Validation Loss: 0.20,  Test Loss: 0.26\n",
            "Train Accuracy: 97.94%, Validation Accuracy: 93.15%, Test Accuracy: 92.43%\n"
          ]
        }
      ],
      "source": [
        "# Compile and fit the model\n",
        "\n",
        "model.compile(\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = Adamax(0.001),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x_train,y_train,\n",
        "    epochs = 20,\n",
        "    verbose = 0,\n",
        "    validation_data=(x_val,y_val)\n",
        ")\n",
        "\n",
        "train_loss, train_accuracy = model.evaluate(x_train, y_train)\n",
        "val_loss, val_accuracy = model.evaluate(x_val, y_val)\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f'Train Loss: {train_loss:.2f}, Validation Loss: {val_loss:.2f},  Test Loss: {test_loss:.2f}')\n",
        "print(f'Train Accuracy: {train_accuracy*100:.2f}%, Validation Accuracy: {val_accuracy*100:.2f}%, Test Accuracy: {test_accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ojukxX96BkTO",
        "outputId": "2466bb0f-a9b0-49ae-bd68-82216a073bfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 98ms/step\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step\n",
            "\n",
            "Train and Test Data Results -\n",
            "  Metric:  Train:  Test:\n",
            " Accuracy   97.94  92.43\n",
            "Precision   98.61  94.06\n",
            "   Recall   98.60  93.08\n",
            " F1 Score   98.60  93.55\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model with other metrics\n",
        "\n",
        "# Train data\n",
        "y_train_predict_ = model.predict(x_train)\n",
        "y_train_predict = np.argmax(tf.nn.softmax(y_train_predict_), axis=1)\n",
        "\n",
        "# Test data\n",
        "y_test_predict_ = model.predict(x_test)\n",
        "y_test_predict = np.argmax(tf.nn.softmax(y_test_predict_), axis=1)\n",
        "\n",
        "# Define the metrics and corresponding values\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "train_values = [accuracy_score(y_train, y_train_predict) * 100,\n",
        "                precision_score(y_train, y_train_predict, average='macro') * 100,\n",
        "                recall_score(y_train, y_train_predict, average='macro') * 100,\n",
        "                f1_score(y_train, y_train_predict, average='macro') * 100]\n",
        "test_values = [accuracy_score(y_test, y_test_predict) * 100,\n",
        "               precision_score(y_test, y_test_predict, average='macro') * 100,\n",
        "               recall_score(y_test, y_test_predict, average='macro') * 100,\n",
        "               f1_score(y_test, y_test_predict, average='macro') * 100]\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Metric:': metrics,\n",
        "    'Train:': train_values,\n",
        "    'Test:': test_values\n",
        "})\n",
        "\n",
        "# Print DataFrame\n",
        "print(\"\\nTrain and Test Data Results -\")\n",
        "print(results_df.round(2).to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
